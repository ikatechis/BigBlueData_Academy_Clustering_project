{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcbf44-7f26-4c5c-b93e-6f6a992cf890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30753f-563d-4b44-bb70-745ae310b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89f611-6ec0-41f6-892c-0bf64b604565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glossary of Comic Terms\n",
    "# https://comics.ha.com/c/ref/glossary.zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ce8e1-2e25-4f9c-a388-9ed494f55b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('../data/all_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eefea7-ce00-401c-a3fa-04d109ee8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a157ae9-83c5-46fe-bbdf-bfb793569fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the proper date info\n",
    "proper_date = pd.read_csv('../data/proper_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43c0fc-0a33-4d0d-b344-bebdbca3b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = proper_date.proper_date.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074df533-8e65-4ffd-bc04-f070a237c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv('../data/titles_list_data.csv', low_memory=False, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61127103-a26c-4e8f-b8d5-e4c1fc6d8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.title.duplicated().value_counts()\n",
    "# there are identical titles so we have to connect them with a unique id for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965059cf-942a-472b-aeb9-1e404e6ff0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract titles' and issues' unique ids\n",
    "data['title_id'] = data.copy().title_link.str.extract(pat=r'/series.(\\d+)/\\d.+')\n",
    "data['issue_id'] = data.copy().issue_link.str.extract(pat=r'/series/\\d+/(\\d+)/.+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab08792-635f-4e2d-b7a1-c14441d6b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep relevant columns\n",
    "df = data[['pub_name', 'title', 'title_id', 'issue', 'variant_of', 'volume', 'cover_date', \n",
    "           'years', 'date', 'cover_price', 'current_value', 'searched', \n",
    "          'owned', 'issues_total', 'est_print_run',  \n",
    "           'pub_titles_total', 'pub_issues_total',\n",
    "           'contributors_names', 'contributors_roles', 'characters', 'synopsis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ee1c1-e380-42bf-b4c0-a7103a226be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn prices into numerical\n",
    "df.loc[:, 'cover_price'] = df.copy().loc[:, 'cover_price'].str.replace('[$,]', '', regex=True)\n",
    "df.loc[:, 'cover_price'] = df.copy().loc[:, 'cover_price'].str.replace('Free', '0', regex=False)\n",
    "df.loc[:, 'cover_price'] = df.copy().loc[:, 'cover_price'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f07e0-16aa-4290-9913-394d78f5fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'current_value'] = df.copy().loc[:, 'current_value'].str.replace('[$,]', '', regex=True)\n",
    "df.loc[:, 'current_value'] = df.copy().loc[:, 'current_value'].str.replace('Free', '0', regex=False)\n",
    "df.loc[:, 'current_value'] = df.copy().loc[:, 'current_value'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f614f44-9af7-42e9-ad71-912c11815bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many issues with cover price 0 have high current value so we keep them\n",
    "df[df.cover_price == 0].sort_values('current_value', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b362899-5aa9-4f31-bd60-28f5cc5f04ce",
   "metadata": {},
   "source": [
    "### `issue` column\n",
    "\n",
    "There are several types of issues as indicated by their numbering and many unconventional numberings:\n",
    "\n",
    "* Simple ascending numbering # 1-...\n",
    "\n",
    "* Issues with numbering of the format: # (number)(variant-cover/ special ed./ convention ed. etc.): Normally these issues should be indicated as \"variant of\" (issue number) in the `variant_of` column\n",
    "\n",
    "* 'Ashcan' Issues which are traditionally used to promote a new series (see [wiki](https://en.wikipedia.org/wiki/Ashcan_comic)). Ashcan comics can be quite rare and valuable, especially from the [Golden Age](https://en.wikipedia.org/wiki/Golden_Age_of_Comic_Books) (30s-50s)\n",
    "\n",
    "* Issues with `nn` meaning 'non-numbered'. Typically 0th issues or one-shots\n",
    "\n",
    "* Of \"Vol ** # **\" format (where ** indicated a number). i.e. two enumerations are indicated, one for volume, one for issue number\n",
    "\n",
    "* Instead of issue number a date is given e.g. \"Spring 2007\" or simply \"1989\"\n",
    "\n",
    "* Numbering which includes letters e.g. \"C-2\" or \"x\"\n",
    "\n",
    "* Just the name of the protagonist or in general something explanatory e.g. \"Batwoman\"\n",
    "\n",
    "* The print run is indicated e.g. '# 2 - 2nd print'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb3578-4583-4730-b6b8-c9edcc2a366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d78ae-837d-4b60-bb10-87758835b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the print number\n",
    "prints = df1.issue.str.extract('.*(\\d{1,2})[a-zA-Z]{2}[\\s\\-][pP]rint')\n",
    "df1['print_nr'] = prints.fillna(1).astype(int)\n",
    "# extract volume and issue information from issue of the type e.g. 'V5-14' or 'V9 6' or 'V12#6'\n",
    "vols = (df1.issue.str.extractall('.*[Vv][oO]?[lL]?(\\d+)[\\s\\-#]+(\\d+)')\n",
    "            .droplevel(1).rename({0: 'vol', 1: 'issue'}, axis=1))\n",
    "# extract volume and issue information from issue of the type e.g. 'B-14' or 'C 6'\n",
    "lett = df1.issue.str.extractall('([A-Z])[#\\s\\-](\\d+)\\s?.*').droplevel(1).rename({0: 'vol', 1: 'issue'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73decf31-16b1-450d-86d2-ff699e4a0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all ashcan issues\n",
    "idx_ash = df1.issue.str.contains('[Aa]shcan')\n",
    "# Find all non-numbered and ashcan issues\n",
    "idx_nn = df1.issue.str.contains('(?:\\s+|^)nn-?(?:\\s*|$)')\n",
    "df1.loc[idx_nn, 'volume'] = 'one-shot'\n",
    "# Find all previews\n",
    "idx_prev = df1.issue.str.contains('[Pp]review')\n",
    "# find all issues that contain also the volume information\n",
    "vols_idx = df1.issue.str.contains('.*[Vv][oO]?[lL]?#?\\d+[\\s\\-#]+\\d+')\n",
    "# find issues that are properly numbered\n",
    "numb_idx = df1.issue.str.contains('.*#\\s*-?\\d+\\D*.*')\n",
    "# find issues that are numbered as e.g. B-5 etc.\n",
    "lett_idx = df1.issue.str.contains('^[A-Z][#\\s\\-]\\d+\\s?.*')\n",
    "# find issues that are numbered by single capital letters\n",
    "caplet_idx = df1.issue.str.contains('^[A-Z]$')\n",
    "# find issues that are numbered by single lowercase letters\n",
    "lowlet_idx = df1.issue.str.contains('^[a-z]$')\n",
    "# find issues from free comic book day\n",
    "fcbd_idx = df1.issue.str.contains('fcbd|free comic')\n",
    "df1.loc[fcbd_idx, 'volume'] = 'fcbd'\n",
    "\n",
    "# Extract issue numbers by the ones that are numbered in the conventional way\n",
    "numbered = df1.issue.str.extract(r'#\\s*(\\d+)\\D*')\n",
    "\n",
    "# ashcan --> -1\n",
    "numbered.loc[idx_ash] = str(0)\n",
    "# nn --> 1\n",
    "numbered.loc[idx_nn] = str(1)\n",
    "# preview --> 0.5\n",
    "numbered.loc[idx_prev] = str(0.5)\n",
    "# free comic book day (fcbd) --> 1\n",
    "numbered.loc[fcbd_idx] = str(1)\n",
    "# single capital letter numbering to int numbers\n",
    "numbered.loc[caplet_idx] = df1[caplet_idx].issue.apply(lambda x: str(ord(x) - 64)).values.reshape(-1, 1)\n",
    "# single lower letter numbering to int numbers\n",
    "numbered.loc[lowlet_idx] = df1[lowlet_idx].issue.apply(lambda x: str(ord(x) - 96)).values.reshape(-1, 1)\n",
    "\n",
    "# add volume and issue info extracted from issue column\n",
    "numbered.loc[vols_idx, 0] = vols.issue \n",
    "df1.loc[vols_idx, 'volume'] = vols.vol\n",
    "numbered.loc[lett_idx, 0] = lett.issue\n",
    "df1.loc[lett_idx, 'volume'] = lett.vol.apply(lambda x: str(ord(x) - 64))\n",
    "\n",
    "# create new column with the issues numbers\n",
    "df1.insert(4, 'issue_nr', pd.to_numeric(numbered[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8666850-aa2e-4845-9279-5401d0aac1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the above\n",
    "regex = r'(?:\\s+|^)nn-?(?:\\s*|$)|.*#\\s*-?\\d+\\D*.*|[Aa]shcan|[Pp]review|.*[Vv]\\d+\\s?-?\\s?\\d+|[A-Z][\\s-]\\d+\\s?.*|^[A-Z]$|fcbd|free comic|^[a-z]$'\n",
    "idx_good = (df1.issue.str.contains(regex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56ee32-4fbc-48c6-b496-0b32eed927c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All that remained that we will drop since issue number is considered essential\n",
    "print(df1[~idx_good].shape)\n",
    "print(df1[~idx_good].current_value.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401a1ca-e473-4889-a3cb-835275f43f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = df1.dropna(subset=['issue_nr'])\n",
    "# drop issues with issue_nr > 10000 - considered outliers\n",
    "df2 = df2[~(df2.issue_nr > 10000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef59874-57c6-45bd-ade4-3f55954645a5",
   "metadata": {},
   "source": [
    "### `volume` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f30e5-2f94-40a2-8070-35dbb250e582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All none values for volume can be considered volume 1\n",
    "df2.volume.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e501477-4b16-4e22-a96e-7a7df08fc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take care of volume numbers formatted as e.g. 'Vol. 5'\n",
    "vol_nr = df2.volume.str.extract('.*[vV][oO][lL][.\\s\\-]+(\\d+)')\n",
    "vol_nr_idx = vol_nr.notna()[0]\n",
    "\n",
    "df2.loc[vol_nr_idx, 'volume'] =  vol_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497d258-7c61-49d3-9d83-401c60d4247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.volume.fillna('0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a2d88-a709-4058-85df-affe9967ab9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Volume of type 'one-shot'\n",
    "oneshot_idx = df2.volume.str.contains('one[\\s\\-=]+shot', flags=re.IGNORECASE)\n",
    "df2.loc[oneshot_idx, 'volume'] = 'one-shot'\n",
    "# Volume of type trade paper-back (TPB) or soft-cover (SC)\n",
    "tpb_idx = df2.volume.str.contains('TRPB|TPB|TP|paperback|sc|soft', flags=re.IGNORECASE)\n",
    "df2.loc[tpb_idx, 'volume'] = 'TPB/SC'\n",
    "# Volume of type Hard Cover (HC)\n",
    "hc_idx = df2.volume.str.contains('HC|hard', flags=re.IGNORECASE)\n",
    "df2.loc[hc_idx, 'volume'] = 'HC'\n",
    "# Volume of type graphic novel (GN)\n",
    "gn_idx = df2.volume.str.contains('^gn$|^ogn$', flags=re.IGNORECASE)\n",
    "df2.loc[gn_idx, 'volume'] = 'GN'\n",
    "# Volume of type mini-series/ maxi-series /limited-series which are ofter used interchangeably (https://en.wikipedia.org/wiki/Limited_series_(comics))\n",
    "mini_idx = df2.volume.str.contains('mini|maxi|limit', flags=re.IGNORECASE)\n",
    "df2.loc[mini_idx, 'volume'] = 'limited-series'\n",
    "# Volume of type magazine\n",
    "mag_idx = df2.volume.str.contains('magazine', flags=re.IGNORECASE)\n",
    "df2.loc[mag_idx, 'volume'] = 'magazine'\n",
    "# Volume of type fanzine\n",
    "fan_idx = df2.volume.str.contains('fanzine', flags=re.IGNORECASE)\n",
    "df2.loc[fan_idx, 'volume'] = 'fanzine'\n",
    "# Volume for fcbd comics\n",
    "FCBD_idx = df2.volume.str.contains('fcbd', flags=re.IGNORECASE)\n",
    "df2.loc[FCBD_idx, 'volume'] = 'fcbd'\n",
    "# Volume for four-colour comics\n",
    "four_idx = df2.volume.str.contains('(?=.*[fF]our)(?=.*[cC]olor|.*[cC]olour)', flags=re.IGNORECASE)\n",
    "df2.loc[four_idx, 'volume'] = 'four-color'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eddd12-8f57-4e70-a6b5-5e1bf23a858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_cats = ['limited-series', 'one-shot', 'magazine', 'TPB/SC', 'HC', 'GN', 'four-color', \n",
    "               'Prepack', 'fcbd', 'fanzine']\n",
    "volume_nums = list(range(21))\n",
    "volume_lst = volume_cats + [str(n) for n in volume_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89ca3a-6e1e-423f-9712-d62ca1f0c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_idx = ~df2.volume.isin(volume_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147413b9-03f0-4b56-b65c-4721fd6000fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of those, in most cases these are not comic books\n",
    "df3 = df2[~other_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28899ba3-498a-4300-ab53-9a3fd20f08e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af76325-8a66-4bd5-b2d2-2456be00c181",
   "metadata": {},
   "source": [
    "### `variant_of` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8bf7c4-0ff5-4d6c-8534-bb65bbc933ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use info in variant to fill volume information\n",
    "variant = df3[df3.variant_of.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204614d-805a-4b41-959a-be9a1ac0b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find variant number in the form 'Soft Cover or TPB # 213'\n",
    "tpb_var_idx = variant.variant_of.str.contains('TRPB|TPB|TP|paperback|sc|soft', flags=re.IGNORECASE)\n",
    "# extract issue number\n",
    "tpb_var_num = variant[tpb_var_idx].variant_of.str.extract(r'.*#?\\s*(\\d+)\\D*')\n",
    "\n",
    "# find variant number in the form 'Hard Cover # 213'\n",
    "hc_var_idx = variant.variant_of.str.contains('hard|HC', flags=re.IGNORECASE)\n",
    "# extract issue number\n",
    "hc_var_num = variant[hc_var_idx].variant_of.str.extract(r'.*#?\\s*(\\d+)\\D*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbf239-863f-49ff-9928-03c11a6ab0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_tpb_index = tpb_var_idx[(tpb_var_idx == True)].index\n",
    "df3.loc[var_tpb_index, 'volume'] = 'TPB/SC'\n",
    "\n",
    "var_hc_index = tpb_var_idx[(hc_var_idx == True)].index\n",
    "df3.loc[var_hc_index, 'volume'] = 'HC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5fcc5-0f98-48c6-999b-2756c1b53476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make new column where variant is bool True or False\n",
    "is_variant = df3.variant_of.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03992326-9b58-441d-9e5e-4520bb979d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3.loc[:, 'variant'] = is_variant.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5aa06-c2cc-4c22-b86f-bb4225103436",
   "metadata": {},
   "source": [
    "### `synopsis` column\n",
    "\n",
    "Try to find other information from the synopsis of each issue e.g. if it is an incentive cover variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8beef0d-516b-4c0f-a0f4-e0c0da639d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_colwidth = 1000\n",
    "# pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc9c96-61cc-4091-ad0b-4f99104735ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.synopsis.notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f1f83-a689-48e6-a2cf-233b181c39ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.synopsis.notna()].synopsis.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bc055-6300-47ba-84fe-9005083851ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "incentives = df3.synopsis.str.contains('incentive').fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0fd16-dfde-4bad-8482-106a6ebde178",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_idx = df3.synopsis.str.contains('(?=.*limited)(?=.*edition|.*cover|.*variant|.*issue|.*copies|.*copy)', flags=re.IGNORECASE).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ebbf9-71e8-40c5-9676-1b31d4ed0926",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_copies = df3[limited_idx].synopsis.str.extract(r'.*\\s(\\d,?\\d+) copies.*|.*limited to (\\d,?\\d+).*|.*1:(\\d,?\\d+).*', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d726211-1c3d-4cd5-b9f2-472b5ee082c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_copies = limit_copies[0].fillna(limit_copies[1].fillna(limit_copies[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c7f56-d257-4e14-87cf-29a034f413df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deluxe edition (< limited)\n",
    "deluxe_idx = df3.synopsis.str.contains('(?=.*deluxe)(?=.*edition)', flags=re.IGNORECASE).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cdb09-91a9-4e0c-a59c-4548f8f77659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column 'special' to indicate limited/deluxe edition etc\n",
    "df3.loc[:, 'special'] = 'not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5330294-5aac-4c18-a566-13d847a35a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[deluxe_idx, 'special'] = 'deluxe'\n",
    "df3.loc[limited_idx, 'special'] = 'limited'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d50db-459b-4ef0-9405-58861ae33d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.special.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a935b-c289-4cb0-b66a-2c68f13aaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of copies for the limited editions wherever it could be obtained\n",
    "limit_copies = limit_copies.str.replace(',', '').astype(float).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d738c-9db4-4ffc-ac53-da855016cc52",
   "metadata": {},
   "source": [
    "#### Get first appearance and maybe other special events from `characters` column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9fc5e-4c43-4e60-8e16-70b4cfa0e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(char_list):\n",
    "    \n",
    "    if char_list == 'no characters assigned':\n",
    "        return 'nothing'\n",
    "    else:\n",
    "        chars = eval(char_list)\n",
    "        events = []\n",
    "        for char in chars:\n",
    "            if char[-1] != '':\n",
    "                events.append(char[-1])\n",
    "            else:\n",
    "                continue\n",
    "        if events == []:\n",
    "            return 'nothing'\n",
    "        else:\n",
    "            return str(events)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691d2e3-09d4-4e55-8865-3a904ad6a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = df3.characters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88a6a8-5eb0-4900-9b4c-cc532dadd290",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d54bdb-b334-4b8e-9dfe-2da466d48829",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = df3.characters.apply(get_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6702007-7719-4007-ab47-79dfc8e16e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[:, 'first_appear_event'] = events.str.contains('First appearance', flags=re.IGNORECASE)\n",
    "df3.loc[:, 'death_event'] = events.str.contains('Death', flags=re.IGNORECASE)\n",
    "df3.loc[:, 'origin_event'] = events.str.contains('origin', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b644b-7ab8-43d5-aad2-6322a4da0425",
   "metadata": {},
   "source": [
    "### Prepare final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b6202-d047-45ae-a341-9afa18fd781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4679a3-6487-453b-b3e3-341843065c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7ba45-e130-4f36-b358-102faa5de800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4[['pub_name', 'title', 'title_id', 'issue_nr', 'variant', 'volume', 'print_nr', \n",
    "          'date', 'cover_price', 'current_value', 'special', 'searched', 'owned',\n",
    "         'first_appear_event', 'death_event', 'origin_event', 'issues_total',\n",
    "         'pub_titles_total', 'pub_issues_total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf220ed6-9c9c-4885-879f-eceb53c17ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e15eeb-b131-45da-9214-3b273c5290f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('../data/data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd9758-73a6-4315-b8d0-509bdae0f900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
