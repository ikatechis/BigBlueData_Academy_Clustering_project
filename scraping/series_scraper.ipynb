{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81fb243-acb7-4367-b983-db09929a292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074e7001-0574-4326-b0a1-737aa03271dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_link = 'https://comicbookrealm.com/series/113/0/marvel-comics-amazing-spider-man-vol-1' # Normally we get this from titles list dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9183bdca-8be2-4b21-b8a6-65e985190edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issues_and_covers(page_data):\n",
    "    \n",
    "    covers = []\n",
    "    issues_link = []\n",
    "    issues_nr = []\n",
    "    # get covers\n",
    "    spans = page_data.find_all('span') \n",
    "    for span in spans:\n",
    "        class_ = span.get('class')\n",
    "\n",
    "        if class_ is None:\n",
    "            continue\n",
    "\n",
    "        elif class_[0] == 'image_approved':\n",
    "            covers.append(span.find('a').get('href'))\n",
    "\n",
    "        else:\n",
    "            covers.append('no_cover')\n",
    "    # get issues\n",
    "    tds = page_data.find_all('td', class_='issue')        \n",
    "    for td in tds:\n",
    "        a = td.find_all('a')[-1]\n",
    "        ref = a.get('href')\n",
    "        txt = a.text\n",
    "        issues_link.append(ref)\n",
    "        issues_nr.append(txt)\n",
    "    \n",
    "    return issues_link, issues_nr, covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37cec103-7878-43ac-a291-671a870c6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_issues_scraper(browser, link):\n",
    "\n",
    "    browser.get(link)\n",
    "    # WebDriverWait(browser, wait_time).until(EC.visibility_of_any_elements_located((By.CLASS_NAME, 'page_1')))\n",
    "    time.sleep(1 + random.random()) \n",
    "    try:\n",
    "        # click on accept cookies and privacy aggreements\n",
    "        browser.find_elements_by_xpath(\"//button[@class=' css-47sehv']\")[0].click()\n",
    "        time.sleep(2) # wait a bit for the page to load\n",
    "        browser.find_elements_by_xpath(\"//a[@class='cc_btn cc_btn_accept_all']\")[0].click()\n",
    "        time.sleep(2) # wait a bit for the page to load\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    soup_file = browser.page_source\n",
    "    soup = BeautifulSoup(soup_file)\n",
    "    # get 1st page of issues which is automatically loaded\n",
    "    page_data = soup.find('form', id='current_comics').find('div', class_='page_1')\n",
    "    issues_link, issues_nr, covers = get_issues_and_covers(page_data) # get all issues and cover links from page 1\n",
    "\n",
    "\n",
    "    \n",
    "    # get all pages that are not diplayed and should be clicked upon if they exist\n",
    "    pages = browser.find_elements_by_xpath(\"//a[@class='g']\")\n",
    "    \n",
    "    if pages != []:\n",
    "\n",
    "        for i, page in zip(range(2, len(pages) + 2), pages):    \n",
    "            # print(f'clicked on page_{i}')\n",
    "            page.click()\n",
    "\n",
    "            # WebDriverWait(browser, wait_time).until(EC.visibility_of_any_elements_located((By.CLASS_NAME, f'page_{i}')))\n",
    "            time.sleep(2.5)\n",
    "            soup_file = browser.page_source\n",
    "            soup = BeautifulSoup(soup_file)\n",
    "            page_data = soup.find('form', id='current_comics').find('div', class_=f'page_{i}')\n",
    "            # print(issues)\n",
    "            data = get_issues_and_covers(page_data)\n",
    "            issues_link.extend(data[0])\n",
    "            issues_nr.extend(data[1])\n",
    "            covers.extend(data[2])\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "          'issue_link': issues_link,\n",
    "          'issue_nr': issues_nr,\n",
    "          'cover_link': covers,\n",
    "        \n",
    "        })\n",
    "    \n",
    "    if df.empty:\n",
    "        df = df.append([np.nan], ignore_index=True).drop(0, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def start_firefox(headless=True, driver_path='./'):\n",
    "    # use selenium webdriver so we can scrape the javascript list. \n",
    "    # geckodriver should be in the path or it should be provided. version should correspond to the firefox version installed\n",
    "    # Link for versions: https://github.com/mozilla/geckodriver/releases\n",
    "    if headless:\n",
    "        from selenium.webdriver.firefox.options import Options\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        browser = webdriver.Firefox(executable_path=driver_path+\"geckodriver.exe\", options=options)\n",
    "    else:\n",
    "        browser = webdriver.Firefox(executable_path=driver_path+\"geckodriver.exe\")\n",
    "        \n",
    "def start_chrome(headless=True, driver_path='./'):\n",
    "    # use selenium webdriver so we can scrape the javascript elements\n",
    "    # chromedriver should be in the path or it should be provided. version should correspond to the chrome version installed\n",
    "    # Link for versions: https://chromedriver.chromium.org/downloads\n",
    "    if headless:\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        opts = Options()\n",
    "        opts.add_argument('--blink-settings=imagesEnabled=false')\n",
    "        opts.add_argument(\" --headless\")\n",
    "        chrome_driver = \"chromedriver.exe\"# Instantiate a webdriver\n",
    "        browser = webdriver.Chrome(options=opts, executable_path=driver_path+chrome_driver)# Load the HTML page\n",
    "    else:\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        opts = Options()\n",
    "        opts.add_argument('--blink-settings=imagesEnabled=false')\n",
    "        chrome_driver = \"chromedriver.exe\"\n",
    "        browser = webdriver.Chrome(options=opts, executable_path=driver_path+chrome_driver)\n",
    "\n",
    "    \n",
    "    return browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffabed2-d5b2-4703-8de6-b10ab06a0503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test function\n",
    "# browser = start_browser2(headless=False)\n",
    "# data = all_issues_scraper(browser, main_link, wait_time=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5137978e-f540-4cf4-bd08-ba608750b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3dadc-c5d2-4604-8002-f5b8170c95c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make the Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3aa0f64-217c-46c2-9bfc-c29b254ad6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('titles_list_data.csv', index_col=0, low_memory=False)\n",
    "df1 = df1[df1.issues != '0']\n",
    "# df1.dropna(inplace=True)\n",
    "# df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6e4aef0-9874-40de-ac3c-438b0c42fdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffdcf7-517b-4380-b4f1-375c9552ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = start_chrome(headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0f516-c3a6-4309-bcdb-94088c2a291a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_link = 'https://comicbookrealm.com'\n",
    "frames_list = []\n",
    "df1 = df1.iloc[13001:]\n",
    "\n",
    "iterates = tqdm(zip(range(df1.shape[0]), df1.pub_id, df1.pub_name, df1.title, df1.title_link))\n",
    "\n",
    "for i, pub_id, pub_name, title, link in iterates:\n",
    "    \n",
    "    \n",
    "    iterates.set_description(f\"Processing: {title} ({pub_name}) - {i + 1} out of {df1.shape[0]}\")\n",
    "    try:\n",
    "        frame = all_issues_scraper(browser, site_link+link)\n",
    "    except:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            frame = all_issues_scraper(browser, site_link+link)\n",
    "        except:\n",
    "            continue\n",
    "    frame.insert(0, 'title_link', link)\n",
    "    frame.insert(0, 'title', title)\n",
    "    frame.insert(0, 'pub_id', pub_id)\n",
    "    frames_list.append(frame)\n",
    "    if i%1000 == 0 and i > 0:\n",
    "        temp = pd.concat(frames_list, axis=0)\n",
    "        temp.to_csv(f'issues_data_{i}.csv')\n",
    "    \n",
    "df = pd.concat(frames_list, axis=0)\n",
    "df.to_csv('issues_data_part1.csv')\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e036fea-9494-469f-9f13-f511f2189ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb2069-7151-4115-b85f-5be614a009aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(frames_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24017f60-0bc6-43c2-8978-59b37dcac3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('issues_data_part1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6907e3-7825-4c40-8045-14ecc210f21f",
   "metadata": {},
   "source": [
    "### Minimal example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea7bc3-90dc-44a1-89bb-3afc96a2f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_link = 'https://comicbookrealm.com/series/113/0/marvel-comics-amazing-spider-man-vol-1'\n",
    "ua = UserAgent()\n",
    "headers = {'User-agent': f'{ua.random}'}\n",
    "response = requests.get(main_link, headers=headers)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2577fa-307b-40af-80db-3e7418e384fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use selenium webdriver so we can scrape the javascript list. geckodriver should be in the path or it should be provided\n",
    "browser = webdriver.Firefox(executable_path=\"geckodriver\")\n",
    "browser.get(main_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1dc28-d2c8-4134-aa65-3aecaf40eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "elm = WebDriverWait(browser, 5).until(EC.element_to_be_clickable((By.CLASS_NAME, 'value')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbc8f3-6ef3-41d8-9807-9299a7c547f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_file = browser.page_source\n",
    "soup = BeautifulSoup(soup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f796c83-4e7a-4db7-b26d-2889719dc274",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = soup.find('form', id='current_comics').find('div', class_='page_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef512b23-059c-47de-872f-17bdd9cadadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = get_issues_and_covers(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a4e8c1-1139-44c1-af4c-35276bc48ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d616d-70a3-492e-a48e-7ebf66731806",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_elements_by_xpath(\"//button[@class=' css-47sehv']\")[0].click()\n",
    "time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e45fbe-f3c4-415a-a33d-3e45b033600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_elements_by_xpath(\"//a[@class='cc_btn cc_btn_accept_all']\")[0].click()\n",
    "time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809da49-86fd-4cd2-8e9a-a78b2733cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = browser.find_elements_by_xpath(\"//a[@class='g']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bf9e7-edb3-4453-b79d-b9139dabe750",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1dace-9d7a-488c-a948-59ca434891a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_data = [page1]\n",
    "\n",
    "\n",
    "for i, page in zip(range(2, len(pages) + 2), pages):    \n",
    "    \n",
    "    page.click()\n",
    "    WebDriverWait(browser, 5).until(EC.visibility_of_element_located((By.CLASS_NAME, f'page_{i}')))\n",
    "    # time.sleep(3)\n",
    "    soup_file = browser.page_source\n",
    "    soup = BeautifulSoup(soup_file)\n",
    "    issues = soup.find('form', id='current_comics').find('div', class_=f'page_{i}')\n",
    "    pages_data.append(get_issues_and_covers(issues))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e18f9-78ae-46ae-9aaf-5065419f0fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pages_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e923e-1e4e-468d-85be-5b9e57d62488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively we could use Chrome for scraping...\n",
    "\n",
    "# Instantiate an Options object\n",
    "# and add the \"--headless\" argument\n",
    "opts = Options()\n",
    "opts.add_argument(\" --headless\")\n",
    "# opts.binary_location= os.getcwd() +'\\\\GoogleChromePortable\\GoogleChromePortable.exe'# Set the location of the webdriver\n",
    "chrome_driver = \"chromedriver.exe\"# Instantiate a webdriver\n",
    "driver = webdriver.Chrome(options=opts, executable_path=chrome_driver)# Load the HTML page\n",
    "driver.get(main_link)# To scrape a url rather than a local file \n",
    "\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c145bff-a3dc-47b6-ad50-fbe010355355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup_file = driver.page_source\n",
    "# soup = BeautifulSoup(soup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88306927-8694-439b-88b3-dc49940f4620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# soup.find('form', id='current_comics').find('div', class_='page_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8dad2b-4922-4beb-98ca-fdcabc4f7da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145e4e0-33af-4840-9880-38fe2bd0d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Options()\n",
    "opts.add_argument(\" --headless\")\n",
    "# opts.binary_location= os.getcwd() +'\\\\GoogleChromePortable\\GoogleChromePortable.exe'# Set the location of the webdriver\n",
    "chrome_driver = \"chromedriver.exe\"# Instantiate a webdriver\n",
    "driver = webdriver.Chrome(options=opts, executable_path=chrome_driver)# Load the HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242297eb-3947-4a07-b541-00412e6f8f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
